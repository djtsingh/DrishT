{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76cfe75e",
   "metadata": {},
   "source": [
    "# DrishT: SSDLite-MobileNetV3 Text Detection Training\n",
    "\n",
    "**Model**: SSDLite320 + MobileNetV3-Large (~3.4M params)  \n",
    "**Dataset**: 7,344 train / 915 val / 915 test images (COCO JSON)  \n",
    "**Categories**: text, license_plate, traffic_sign, autorickshaw, tempo, truck, bus  \n",
    "**GPU**: Kaggle T4 (free, 30h/week)  \n",
    "\n",
    "## Setup\n",
    "1. Add dataset `drisht-detection` to this notebook\n",
    "2. Enable GPU: Settings → Accelerator → GPU T4 x2\n",
    "3. Run all cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3199be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, time, shutil\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import autocast, GradScaler\n",
    "import torchvision\n",
    "from torchvision.models.detection import ssdlite320_mobilenet_v3_large\n",
    "from torchvision.models import MobileNet_V3_Large_Weights\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ce1538",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db830a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Adjust this path based on your Kaggle dataset name ---\n",
    "DATA_ROOT = Path('/kaggle/input/drisht-detection')\n",
    "OUTPUT_DIR = Path('/kaggle/working/detection_output')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Paths\n",
    "TRAIN_JSON = DATA_ROOT / 'train' / 'annotations.json'\n",
    "TRAIN_IMAGES = DATA_ROOT / 'train' / 'images'\n",
    "VAL_JSON = DATA_ROOT / 'val' / 'annotations.json'\n",
    "VAL_IMAGES = DATA_ROOT / 'val' / 'images'\n",
    "\n",
    "# Model\n",
    "NUM_CLASSES = 8  # 7 categories + background\n",
    "INPUT_SIZE = 320\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 4   # Kaggle T4 has 4 vCPUs — use all of them\n",
    "EPOCHS = 80\n",
    "LR = 0.01\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 4e-5\n",
    "LR_MIN = 1e-6\n",
    "FREEZE_BACKBONE_EPOCHS = 5\n",
    "PATIENCE = 12\n",
    "USE_AMP = True\n",
    "\n",
    "# Category names\n",
    "CATEGORIES = {\n",
    "    0: 'background', 1: 'text', 2: 'license_plate', 3: 'traffic_sign',\n",
    "    4: 'autorickshaw', 5: 'tempo', 6: 'truck', 7: 'bus'\n",
    "}\n",
    "\n",
    "# Verify data\n",
    "for p in [TRAIN_JSON, VAL_JSON]:\n",
    "    assert p.exists(), f'Missing: {p}'\n",
    "print(f'Train images: {len(list(TRAIN_IMAGES.iterdir()))}')\n",
    "print(f'Val images: {len(list(VAL_IMAGES.iterdir()))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38ba1aa",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eef171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class COCODetectionDataset(Dataset):\n",
    "    \"\"\"COCO JSON format dataset for torchvision detection models.\"\"\"\n",
    "\n",
    "    def __init__(self, json_path, img_dir, augment=False, input_size=320):\n",
    "        with open(json_path, 'r') as f:\n",
    "            coco = json.load(f)\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.input_size = input_size\n",
    "        self.augment = augment\n",
    "        self.images = {img['id']: img for img in coco['images']}\n",
    "        self.img_to_anns = defaultdict(list)\n",
    "        for ann in coco['annotations']:\n",
    "            self.img_to_anns[ann['image_id']].append(ann)\n",
    "        self.img_ids = [iid for iid in self.images if len(self.img_to_anns[iid]) > 0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        img_info = self.images[img_id]\n",
    "        image = Image.open(self.img_dir / img_info['file_name']).convert('RGB')\n",
    "        orig_w, orig_h = image.size\n",
    "\n",
    "        boxes, labels = [], []\n",
    "        for ann in self.img_to_anns[img_id]:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            if w <= 0 or h <= 0: continue\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(ann['category_id'])\n",
    "\n",
    "        if boxes:\n",
    "            boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.tensor(labels, dtype=torch.int64)\n",
    "        else:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "\n",
    "        if self.augment:\n",
    "            if random.random() < 0.5:\n",
    "                image = TF.hflip(image)\n",
    "                if len(boxes) > 0:\n",
    "                    boxes[:, [0, 2]] = orig_w - boxes[:, [2, 0]]\n",
    "            if random.random() < 0.5:\n",
    "                image = T.ColorJitter(0.3, 0.3, 0.3, 0.1)(image)\n",
    "\n",
    "        scale_x = self.input_size / orig_w\n",
    "        scale_y = self.input_size / orig_h\n",
    "        image = image.resize((self.input_size, self.input_size), Image.BILINEAR)\n",
    "\n",
    "        if len(boxes) > 0:\n",
    "            boxes[:, [0, 2]] *= scale_x\n",
    "            boxes[:, [1, 3]] *= scale_y\n",
    "            boxes[:, [0, 2]] = boxes[:, [0, 2]].clamp(0, self.input_size)\n",
    "            boxes[:, [1, 3]] = boxes[:, [1, 3]].clamp(0, self.input_size)\n",
    "\n",
    "        image = TF.to_tensor(image)\n",
    "        return image, {'boxes': boxes, 'labels': labels}\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, targets = zip(*batch)\n",
    "    return list(images), list(targets)\n",
    "\n",
    "\n",
    "train_ds = COCODetectionDataset(TRAIN_JSON, TRAIN_IMAGES, augment=True, input_size=INPUT_SIZE)\n",
    "val_ds = COCODetectionDataset(VAL_JSON, VAL_IMAGES, augment=False, input_size=INPUT_SIZE)\n",
    "\n",
    "# num_workers=4 (all Kaggle vCPUs), persistent_workers avoids respawning,\n",
    "# prefetch_factor pre-stages batches so GPU never starves\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, collate_fn=collate_fn,\n",
    "                          pin_memory=True, persistent_workers=True, prefetch_factor=3)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        num_workers=NUM_WORKERS, collate_fn=collate_fn,\n",
    "                        pin_memory=True, persistent_workers=True, prefetch_factor=3)\n",
    "\n",
    "print(f'Train: {len(train_ds)} images, {len(train_loader)} batches')\n",
    "print(f'Val: {len(val_ds)} images, {len(val_loader)} batches')\n",
    "print(f'DataLoader: workers={NUM_WORKERS}, persistent=True, prefetch=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b97989a",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64667d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ssdlite320_mobilenet_v3_large(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    weights_backbone=MobileNet_V3_Large_Weights.IMAGENET1K_V1,\n",
    ")\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total params: {total:,}')\n",
    "print(f'Trainable:    {trainable:,}')\n",
    "print(f'Size:         {sum(p.numel() * p.element_size() for p in model.parameters()) / 1024**2:.1f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4893ce",
   "metadata": {},
   "source": [
    "## Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d0f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_backbone(model, freeze=True):\n",
    "    for param in model.backbone.parameters():\n",
    "        param.requires_grad = not freeze\n",
    "\n",
    "\n",
    "def _box_iou_single(box1, box2):\n",
    "    x1 = max(box1[0].item(), box2[0].item())\n",
    "    y1 = max(box1[1].item(), box2[1].item())\n",
    "    x2 = min(box1[2].item(), box2[2].item())\n",
    "    y2 = min(box1[3].item(), box2[3].item())\n",
    "    inter = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    area1 = (box1[2] - box1[0]).item() * (box1[3] - box1[1]).item()\n",
    "    area2 = (box2[2] - box2[0]).item() * (box2[3] - box2[1]).item()\n",
    "    return inter / max(area1 + area2 - inter, 1e-6)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_map(model, loader, device, iou_threshold=0.5, max_batches=50):\n",
    "    model.eval()\n",
    "    all_dets = defaultdict(list)\n",
    "    all_n_gt = defaultdict(int)\n",
    "\n",
    "    for batch_idx, (images, targets) in enumerate(tqdm(loader, desc='mAP')):\n",
    "        if batch_idx >= max_batches: break\n",
    "        images = [img.to(device) for img in images]\n",
    "        preds = model(images)\n",
    "\n",
    "        for pred, gt in zip(preds, targets):\n",
    "            gt_boxes = gt['boxes'].to(device)\n",
    "            gt_labels = gt['labels'].to(device)\n",
    "            for lbl in gt_labels.tolist():\n",
    "                all_n_gt[lbl] += 1\n",
    "\n",
    "            matched = set()\n",
    "            for i in range(len(pred['boxes'])):\n",
    "                cls = pred['labels'][i].item()\n",
    "                score = pred['scores'][i].item()\n",
    "                best_iou, best_idx = 0.0, -1\n",
    "                for gi in (gt_labels == cls).nonzero(as_tuple=True)[0].tolist():\n",
    "                    if gi in matched: continue\n",
    "                    iou = _box_iou_single(pred['boxes'][i], gt_boxes[gi])\n",
    "                    if iou > best_iou:\n",
    "                        best_iou, best_idx = iou, gi\n",
    "                is_tp = best_iou >= iou_threshold and best_idx >= 0\n",
    "                if is_tp: matched.add(best_idx)\n",
    "                all_dets[cls].append((score, is_tp))\n",
    "\n",
    "    aps = {}\n",
    "    for cls in sorted(all_n_gt):\n",
    "        dets = sorted(all_dets.get(cls, []), key=lambda x: -x[0])\n",
    "        n_gt = all_n_gt[cls]\n",
    "        if n_gt == 0: continue\n",
    "        tp, fp = 0, 0\n",
    "        prec, rec = [], []\n",
    "        for _, is_tp in dets:\n",
    "            tp += is_tp; fp += not is_tp\n",
    "            prec.append(tp / (tp + fp)); rec.append(tp / n_gt)\n",
    "        ap = sum(max((p for p, r in zip(prec, rec) if r >= t), default=0) for t in [i/10 for i in range(11)]) / 11\n",
    "        aps[cls] = ap\n",
    "\n",
    "    return sum(aps.values()) / max(len(aps), 1), aps\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, scaler, device, epoch):\n",
    "    model.train()\n",
    "    total_loss, n = 0.0, 0\n",
    "    pbar = tqdm(loader, desc=f'Epoch {epoch}')\n",
    "    for images, targets in pbar:\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        optimizer.zero_grad()\n",
    "        if scaler:\n",
    "            with autocast('cuda'):\n",
    "                loss_dict = model(images, targets)\n",
    "                loss = sum(loss_dict.values())\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss_dict = model(images, targets)\n",
    "            loss = sum(loss_dict.values())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        total_loss += loss.item(); n += 1\n",
    "        pbar.set_postfix(loss=f'{loss.item():.4f}')\n",
    "    return total_loss / max(n, 1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def val_loss(model, loader, device):\n",
    "    model.train()  # Need train mode for loss\n",
    "    total, n = 0.0, 0\n",
    "    for images, targets in tqdm(loader, desc='Val'):\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        loss = sum(model(images, targets).values())\n",
    "        total += loss.item(); n += 1\n",
    "    return total / max(n, 1)\n",
    "\n",
    "print('Utilities defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c2e82b",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef5ef8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze backbone initially\n",
    "freeze_backbone(model, freeze=True)\n",
    "\n",
    "# Optimizer with separate param groups\n",
    "head_params = [p for n, p in model.named_parameters() if not n.startswith('backbone') and p.requires_grad]\n",
    "backbone_params = [p for p in model.backbone.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = optim.SGD([\n",
    "    {'params': head_params, 'lr': LR},\n",
    "    {'params': backbone_params, 'lr': LR * 0.1},\n",
    "], momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=LR_MIN)\n",
    "scaler = GradScaler('cuda') if USE_AMP and DEVICE.type == 'cuda' else None\n",
    "\n",
    "best_val = float('inf')\n",
    "best_map = 0.0\n",
    "patience_ctr = 0\n",
    "history = []\n",
    "\n",
    "print(f'Training for {EPOCHS} epochs on {DEVICE}')\n",
    "print(f'AMP: {scaler is not None}, Backbone frozen for first {FREEZE_BACKBONE_EPOCHS} epochs')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5f4dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Unfreeze backbone\n",
    "    if epoch == FREEZE_BACKBONE_EPOCHS + 1:\n",
    "        freeze_backbone(model, freeze=False)\n",
    "        backbone_params = list(model.backbone.parameters())\n",
    "        head_params = [p for n, p in model.named_parameters() if not n.startswith('backbone')]\n",
    "        optimizer = optim.SGD([\n",
    "            {'params': head_params, 'lr': scheduler.get_last_lr()[0]},\n",
    "            {'params': backbone_params, 'lr': scheduler.get_last_lr()[0] * 0.1},\n",
    "        ], momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS - epoch + 1, eta_min=LR_MIN)\n",
    "        print(f'  >> Backbone unfrozen at epoch {epoch}')\n",
    "\n",
    "    tloss = train_one_epoch(model, train_loader, optimizer, scaler, DEVICE, epoch)\n",
    "    vloss = val_loss(model, val_loader, DEVICE)\n",
    "\n",
    "    # mAP every 5 epochs\n",
    "    mAP = 0.0\n",
    "    if epoch % 5 == 0 or epoch == EPOCHS:\n",
    "        mAP, per_cls = compute_map(model, val_loader, DEVICE)\n",
    "        ap_str = ' | '.join(f'{CATEGORIES.get(c,c)}: {ap:.3f}' for c, ap in sorted(per_cls.items()))\n",
    "        print(f'  mAP@0.5: {mAP:.4f}  [{ap_str}]')\n",
    "\n",
    "    scheduler.step()\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    elapsed = time.time() - t0\n",
    "\n",
    "    print(f'Epoch {epoch:3d} | Train: {tloss:.4f} | Val: {vloss:.4f} | mAP: {mAP:.4f} | LR: {lr:.6f} | {elapsed:.1f}s')\n",
    "    history.append({'epoch': epoch, 'train_loss': tloss, 'val_loss': vloss, 'mAP': mAP, 'lr': lr})\n",
    "\n",
    "    # Save best\n",
    "    if vloss < best_val:\n",
    "        best_val = vloss\n",
    "        patience_ctr = 0\n",
    "        torch.save({'model': model.state_dict(), 'epoch': epoch, 'best_val': best_val, 'best_map': best_map},\n",
    "                   OUTPUT_DIR / 'best.pth')\n",
    "        print(f'  -> Saved best model (val_loss={vloss:.4f})')\n",
    "    else:\n",
    "        patience_ctr += 1\n",
    "\n",
    "    if mAP > best_map:\n",
    "        best_map = mAP\n",
    "        torch.save({'model': model.state_dict(), 'epoch': epoch, 'best_map': best_map},\n",
    "                   OUTPUT_DIR / 'best_map.pth')\n",
    "        print(f'  -> Saved best mAP model ({mAP:.4f})')\n",
    "\n",
    "    # Checkpoint every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        torch.save({'model': model.state_dict(), 'epoch': epoch},\n",
    "                   OUTPUT_DIR / f'epoch_{epoch}.pth')\n",
    "\n",
    "    if patience_ctr >= PATIENCE:\n",
    "        print(f'\\nEarly stopping at epoch {epoch}')\n",
    "        break\n",
    "\n",
    "print(f'\\nDone! Best val_loss: {best_val:.4f}, Best mAP: {best_map:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ce1641",
   "metadata": {},
   "source": [
    "## Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c029a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = [h['epoch'] for h in history]\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(epochs, [h['train_loss'] for h in history], label='Train')\n",
    "ax1.plot(epochs, [h['val_loss'] for h in history], label='Val')\n",
    "ax1.set_xlabel('Epoch'); ax1.set_ylabel('Loss'); ax1.legend(); ax1.set_title('Loss')\n",
    "\n",
    "map_epochs = [h['epoch'] for h in history if h['mAP'] > 0]\n",
    "map_vals = [h['mAP'] for h in history if h['mAP'] > 0]\n",
    "ax2.plot(map_epochs, map_vals, 'go-')\n",
    "ax2.set_xlabel('Epoch'); ax2.set_ylabel('mAP@0.5'); ax2.set_title('mAP')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'training_curves.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767bb71c",
   "metadata": {},
   "source": [
    "## Export ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad77e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and export to ONNX for mobile deployment\n",
    "best_ckpt = torch.load(OUTPUT_DIR / 'best_map.pth', map_location='cpu')\n",
    "model_export = ssdlite320_mobilenet_v3_large(num_classes=NUM_CLASSES)\n",
    "model_export.load_state_dict(best_ckpt['model'])\n",
    "model_export.eval()\n",
    "\n",
    "dummy = [torch.randn(3, 320, 320)]\n",
    "torch.onnx.export(\n",
    "    model_export, dummy, str(OUTPUT_DIR / 'ssdlite_detection.onnx'),\n",
    "    opset_version=17,\n",
    "    input_names=['image'],\n",
    "    output_names=['boxes', 'labels', 'scores'],\n",
    ")\n",
    "onnx_size = (OUTPUT_DIR / 'ssdlite_detection.onnx').stat().st_size / 1024**2\n",
    "print(f'ONNX exported: {onnx_size:.1f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f56b516",
   "metadata": {},
   "source": [
    "## Save History & Download\n",
    "Download the output files from `/kaggle/working/detection_output/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33b9662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(OUTPUT_DIR / 'history.json', 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "print('Output files:')\n",
    "for f in sorted(OUTPUT_DIR.iterdir()):\n",
    "    print(f'  {f.name}: {f.stat().st_size / 1024**2:.1f} MB')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
