{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76cfe75e",
   "metadata": {},
   "source": [
    "# DrishT: SSDLite-MobileNetV3 Text Detection Training\n",
    "\n",
    "**Model**: SSDLite320 + MobileNetV3-Large (~3.4M params)  \n",
    "**Dataset**: 7,344 train / 915 val / 915 test images (COCO JSON)  \n",
    "**Categories**: text, license_plate, traffic_sign, autorickshaw, tempo, truck, bus  \n",
    "**GPU**: Kaggle T4 (free, 30h/week)  \n",
    "\n",
    "## Setup\n",
    "1. Add dataset `djt5ingh/drisht-detection` to this notebook\n",
    "2. Enable GPU: Settings → Accelerator → GPU T4 x2\n",
    "3. Run all cells\n",
    "\n",
    "## Key Design Decisions\n",
    "- **Pre-loaded tensor**: All images loaded into uint8 RAM tensor at init (~2.1 GB) → zero I/O during training\n",
    "- **Backbone freeze**: First 5 epochs train only the SSD head, then full fine-tuning\n",
    "- **Gradient clipping**: Max norm 10.0 to prevent exploding gradients from SSD loss\n",
    "- **Resumable checkpoints**: Every 10 epochs with optimizer/scheduler/scaler state\n",
    "- **AMP**: Full fp16 (SSD loss is stable in fp16, unlike CTC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3199be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, time, shutil, random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import autocast, GradScaler\n",
    "import torchvision\n",
    "from torchvision.models.detection import ssdlite320_mobilenet_v3_large\n",
    "from torchvision.models import MobileNet_V3_Large_Weights\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = False  # True hurts perf\n",
    "    torch.backends.cudnn.benchmark = True        # Autotune conv algos for fixed 320x320\n",
    "\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB')\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ce1538",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db830a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Auto-discover data root ---\n",
    "OUTPUT_DIR = Path('/kaggle/working/detection_output')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# DIAGNOSTIC: Show what's actually mounted\n",
    "_INPUT = Path('/kaggle/input')\n",
    "if _INPUT.exists():\n",
    "    mounted = sorted(d.name for d in _INPUT.iterdir())\n",
    "    print(f'Mounted datasets: {mounted}')\n",
    "else:\n",
    "    mounted = []\n",
    "    print('WARNING: /kaggle/input does not exist (not running on Kaggle?)')\n",
    "\n",
    "# Search ALL mounted datasets for our annotations.json\n",
    "DATA_ROOT = None\n",
    "for base in (_INPUT.iterdir() if _INPUT.exists() else []):\n",
    "    # Check flat: /kaggle/input/<slug>/train/annotations.json\n",
    "    if (base / 'train' / 'annotations.json').exists():\n",
    "        DATA_ROOT = base\n",
    "        break\n",
    "    # Check nested: /kaggle/input/<slug>/<subfolder>/train/annotations.json\n",
    "    if base.is_dir():\n",
    "        for sub in base.iterdir():\n",
    "            if sub.is_dir() and (sub / 'train' / 'annotations.json').exists():\n",
    "                DATA_ROOT = sub\n",
    "                break\n",
    "    if DATA_ROOT:\n",
    "        break\n",
    "\n",
    "if DATA_ROOT is None:\n",
    "    drisht = [d for d in mounted if 'drisht' in d.lower() or 'detect' in d.lower()]\n",
    "    raise FileNotFoundError(\n",
    "        f'Cannot find train/annotations.json under /kaggle/input/.\\n'\n",
    "        f'Mounted datasets: {mounted}\\n'\n",
    "        f'Possible matches: {drisht}\\n\\n'\n",
    "        f'FIX: Make sure you:\\n'\n",
    "        f'  1. Click \"Add Input\" (right sidebar) → search \"drisht-detection\" → Add\\n'\n",
    "        f'  2. RESTART the kernel session (adding data requires restart)\\n'\n",
    "        f'  3. Re-run all cells'\n",
    "    )\n",
    "\n",
    "print(f'DATA_ROOT: {DATA_ROOT}')\n",
    "\n",
    "# Paths\n",
    "TRAIN_JSON = DATA_ROOT / 'train' / 'annotations.json'\n",
    "TRAIN_IMAGES = DATA_ROOT / 'train' / 'images'\n",
    "VAL_JSON = DATA_ROOT / 'val' / 'annotations.json'\n",
    "VAL_IMAGES = DATA_ROOT / 'val' / 'images'\n",
    "\n",
    "# Model\n",
    "NUM_CLASSES = 8  # 7 categories + background\n",
    "INPUT_SIZE = 320\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 32    # SSD320 is small enough for T4 at bs=32\n",
    "NUM_WORKERS = 0    # Data pre-loaded into RAM — no I/O to parallelize\n",
    "EPOCHS = 80\n",
    "LR = 0.01\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 4e-5\n",
    "LR_MIN = 1e-6\n",
    "FREEZE_BACKBONE_EPOCHS = 5\n",
    "PATIENCE = 12\n",
    "USE_AMP = True\n",
    "\n",
    "# Category names\n",
    "CATEGORIES = {\n",
    "    0: 'background', 1: 'text', 2: 'license_plate', 3: 'traffic_sign',\n",
    "    4: 'autorickshaw', 5: 'tempo', 6: 'truck', 7: 'bus'\n",
    "}\n",
    "\n",
    "# Verify data\n",
    "for p in [TRAIN_JSON, VAL_JSON]:\n",
    "    assert p.exists(), f'Missing: {p}'\n",
    "print(f'Train images: {len(list(TRAIN_IMAGES.iterdir()))}')\n",
    "print(f'Val images: {len(list(VAL_IMAGES.iterdir()))}')\n",
    "print(f'Workers: {NUM_WORKERS} (all data pre-loaded into RAM tensors)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38ba1aa",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eef171",
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCODetectionDataset(Dataset):\n",
    "    \"\"\"Pre-loads ALL images into a uint8 tensor at init — zero disk I/O during training.\n",
    "    7,344 images × 3 × 320 × 320 × 1 byte ≈ 2.1 GB (fits in Kaggle's 13 GB RAM).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, json_path, img_dir, augment=False, input_size=320):\n",
    "        with open(json_path, 'r') as f:\n",
    "            coco = json.load(f)\n",
    "        self.input_size = input_size\n",
    "        self.augment = augment\n",
    "\n",
    "        images_meta = {img['id']: img for img in coco['images']}\n",
    "        img_to_anns = defaultdict(list)\n",
    "        for ann in coco['annotations']:\n",
    "            img_to_anns[ann['image_id']].append(ann)\n",
    "        img_ids = [iid for iid in images_meta if len(img_to_anns[iid]) > 0]\n",
    "\n",
    "        # Pre-load ALL images into a single uint8 tensor\n",
    "        n = len(img_ids)\n",
    "        self.data = torch.zeros(n, 3, input_size, input_size, dtype=torch.uint8)\n",
    "        self.targets = []  # List of (boxes_tensor, labels_tensor)\n",
    "        img_dir = Path(img_dir)\n",
    "\n",
    "        print(f'  Pre-loading {n} images into tensor...')\n",
    "        t0 = time.time()\n",
    "        for i, iid in enumerate(img_ids):\n",
    "            img_info = images_meta[iid]\n",
    "            try:\n",
    "                image = Image.open(img_dir / img_info['file_name']).convert('RGB')\n",
    "                orig_w, orig_h = image.size\n",
    "                image = image.resize((input_size, input_size), Image.BILINEAR)\n",
    "                arr = np.array(image)  # (H, W, 3) uint8\n",
    "                self.data[i] = torch.from_numpy(arr.transpose(2, 0, 1))\n",
    "\n",
    "                # Pre-scale boxes to input_size coordinates\n",
    "                scale_x = input_size / orig_w\n",
    "                scale_y = input_size / orig_h\n",
    "                boxes, labels = [], []\n",
    "                for ann in img_to_anns[iid]:\n",
    "                    x, y, w, h = ann['bbox']\n",
    "                    if w <= 0 or h <= 0: continue\n",
    "                    x1 = max(0.0, x * scale_x)\n",
    "                    y1 = max(0.0, y * scale_y)\n",
    "                    x2 = min(float(input_size), (x + w) * scale_x)\n",
    "                    y2 = min(float(input_size), (y + h) * scale_y)\n",
    "                    if x2 - x1 > 1 and y2 - y1 > 1:\n",
    "                        boxes.append([x1, y1, x2, y2])\n",
    "                        labels.append(ann['category_id'])\n",
    "\n",
    "                if boxes:\n",
    "                    self.targets.append((\n",
    "                        torch.tensor(boxes, dtype=torch.float32),\n",
    "                        torch.tensor(labels, dtype=torch.int64),\n",
    "                    ))\n",
    "                else:\n",
    "                    self.targets.append((\n",
    "                        torch.zeros((0, 4), dtype=torch.float32),\n",
    "                        torch.zeros((0,), dtype=torch.int64),\n",
    "                    ))\n",
    "            except Exception:\n",
    "                self.targets.append((\n",
    "                    torch.zeros((0, 4), dtype=torch.float32),\n",
    "                    torch.zeros((0,), dtype=torch.int64),\n",
    "                ))\n",
    "\n",
    "            if (i + 1) % 2000 == 0:\n",
    "                print(f'    {i+1}/{n} ({time.time()-t0:.0f}s)')\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "        mb = self.data.nbytes / 1024**2\n",
    "        print(f'  Done: {self.data.shape} uint8 tensor, {mb:.0f} MB, {elapsed:.1f}s')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # uint8 → float32 [0, 1] (torchvision detection models expect this range)\n",
    "        image = self.data[idx].float().div_(255.0)\n",
    "        boxes, labels = self.targets[idx]\n",
    "        boxes = boxes.clone()  # Don't mutate stored data\n",
    "\n",
    "        if self.augment:\n",
    "            # Horizontal flip\n",
    "            if random.random() < 0.5:\n",
    "                image = image.flip(-1)\n",
    "                if boxes.numel() > 0:\n",
    "                    boxes[:, [0, 2]] = self.input_size - boxes[:, [2, 0]]\n",
    "\n",
    "            # Brightness\n",
    "            if random.random() < 0.3:\n",
    "                image.mul_(random.uniform(0.7, 1.3))\n",
    "\n",
    "            # Contrast\n",
    "            if random.random() < 0.3:\n",
    "                mean = image.mean()\n",
    "                image = (image - mean).mul_(random.uniform(0.8, 1.2)).add_(mean)\n",
    "\n",
    "            # Saturation\n",
    "            if random.random() < 0.2:\n",
    "                gray = image.mean(dim=0, keepdim=True)\n",
    "                factor = random.uniform(0.7, 1.3)\n",
    "                image = image * factor + gray * (1 - factor)\n",
    "\n",
    "            image.clamp_(0.0, 1.0)\n",
    "\n",
    "            # Filter degenerate boxes after flip\n",
    "            if boxes.numel() > 0:\n",
    "                widths = boxes[:, 2] - boxes[:, 0]\n",
    "                heights = boxes[:, 3] - boxes[:, 1]\n",
    "                valid = (widths > 1) & (heights > 1)\n",
    "                boxes = boxes[valid]\n",
    "                labels = labels[valid]\n",
    "\n",
    "        return image, {'boxes': boxes, 'labels': labels}\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, targets = zip(*batch)\n",
    "    return list(images), list(targets)\n",
    "\n",
    "\n",
    "print('Building datasets (pre-loading images into RAM)...')\n",
    "train_ds = COCODetectionDataset(TRAIN_JSON, TRAIN_IMAGES, augment=True, input_size=INPUT_SIZE)\n",
    "val_ds = COCODetectionDataset(VAL_JSON, VAL_IMAGES, augment=False, input_size=INPUT_SIZE)\n",
    "\n",
    "# num_workers=0: data in RAM tensors, no I/O to parallelize\n",
    "# pin_memory=True: faster CPU→GPU transfer\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, collate_fn=collate_fn,\n",
    "                          pin_memory=True, drop_last=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        num_workers=NUM_WORKERS, collate_fn=collate_fn,\n",
    "                        pin_memory=True)\n",
    "\n",
    "print(f'\\nTrain: {len(train_ds)} images, {len(train_loader)} batches (bs={BATCH_SIZE}, drop_last=True)')\n",
    "print(f'Val: {len(val_ds)} images, {len(val_loader)} batches')\n",
    "print(f'Workers: {NUM_WORKERS} (data pre-loaded, no multiprocessing needed)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b97989a",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64667d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ssdlite320_mobilenet_v3_large(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    weights_backbone=MobileNet_V3_Large_Weights.IMAGENET1K_V1,\n",
    ")\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total params: {total:,}')\n",
    "print(f'Trainable:    {trainable:,}')\n",
    "print(f'Size:         {sum(p.numel() * p.element_size() for p in model.parameters()) / 1024**2:.1f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4893ce",
   "metadata": {},
   "source": [
    "## Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d0f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_backbone(model, freeze=True):\n",
    "    for param in model.backbone.parameters():\n",
    "        param.requires_grad = not freeze\n",
    "\n",
    "\n",
    "def _box_iou_single(box1, box2):\n",
    "    x1 = max(box1[0].item(), box2[0].item())\n",
    "    y1 = max(box1[1].item(), box2[1].item())\n",
    "    x2 = min(box1[2].item(), box2[2].item())\n",
    "    y2 = min(box1[3].item(), box2[3].item())\n",
    "    inter = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    area1 = (box1[2] - box1[0]).item() * (box1[3] - box1[1]).item()\n",
    "    area2 = (box2[2] - box2[0]).item() * (box2[3] - box2[1]).item()\n",
    "    return inter / max(area1 + area2 - inter, 1e-6)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_map(model, loader, device, iou_threshold=0.5, max_batches=50):\n",
    "    model.eval()\n",
    "    all_dets = defaultdict(list)\n",
    "    all_n_gt = defaultdict(int)\n",
    "\n",
    "    for batch_idx, (images, targets) in enumerate(tqdm(loader, desc='mAP', leave=False)):\n",
    "        if batch_idx >= max_batches: break\n",
    "        images = [img.to(device, non_blocking=True) for img in images]\n",
    "        preds = model(images)\n",
    "\n",
    "        for pred, gt in zip(preds, targets):\n",
    "            gt_boxes = gt['boxes'].to(device)\n",
    "            gt_labels = gt['labels'].to(device)\n",
    "            for lbl in gt_labels.tolist():\n",
    "                all_n_gt[lbl] += 1\n",
    "\n",
    "            matched = set()\n",
    "            for i in range(len(pred['boxes'])):\n",
    "                cls = pred['labels'][i].item()\n",
    "                score = pred['scores'][i].item()\n",
    "                best_iou, best_idx = 0.0, -1\n",
    "                for gi in (gt_labels == cls).nonzero(as_tuple=True)[0].tolist():\n",
    "                    if gi in matched: continue\n",
    "                    iou = _box_iou_single(pred['boxes'][i], gt_boxes[gi])\n",
    "                    if iou > best_iou:\n",
    "                        best_iou, best_idx = iou, gi\n",
    "                is_tp = best_iou >= iou_threshold and best_idx >= 0\n",
    "                if is_tp: matched.add(best_idx)\n",
    "                all_dets[cls].append((score, is_tp))\n",
    "\n",
    "    aps = {}\n",
    "    for cls in sorted(all_n_gt):\n",
    "        dets = sorted(all_dets.get(cls, []), key=lambda x: -x[0])\n",
    "        n_gt = all_n_gt[cls]\n",
    "        if n_gt == 0: continue\n",
    "        tp, fp = 0, 0\n",
    "        prec, rec = [], []\n",
    "        for _, is_tp in dets:\n",
    "            tp += is_tp; fp += not is_tp\n",
    "            prec.append(tp / (tp + fp)); rec.append(tp / n_gt)\n",
    "        ap = sum(max((p for p, r in zip(prec, rec) if r >= t), default=0) for t in [i/10 for i in range(11)]) / 11\n",
    "        aps[cls] = ap\n",
    "\n",
    "    return sum(aps.values()) / max(len(aps), 1), aps\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, scaler, device, epoch):\n",
    "    model.train()\n",
    "    total_loss, n = 0.0, 0\n",
    "    pbar = tqdm(loader, desc=f'Epoch {epoch}')\n",
    "    for images, targets in pbar:\n",
    "        images = [img.to(device, non_blocking=True) for img in images]\n",
    "        targets = [{k: v.to(device, non_blocking=True) for k, v in t.items()} for t in targets]\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        if scaler:\n",
    "            with autocast('cuda'):\n",
    "                loss_dict = model(images, targets)\n",
    "                loss = sum(loss_dict.values())\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 10.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss_dict = model(images, targets)\n",
    "            loss = sum(loss_dict.values())\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 10.0)\n",
    "            optimizer.step()\n",
    "        total_loss += loss.item(); n += 1\n",
    "        pbar.set_postfix(loss=f'{loss.item():.4f}')\n",
    "    return total_loss / max(n, 1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def val_loss(model, loader, device, use_amp=False):\n",
    "    model.train()  # Need train mode for SSD loss computation\n",
    "    total, n = 0.0, 0\n",
    "    for images, targets in tqdm(loader, desc='Val', leave=False):\n",
    "        images = [img.to(device, non_blocking=True) for img in images]\n",
    "        targets = [{k: v.to(device, non_blocking=True) for k, v in t.items()} for t in targets]\n",
    "        if use_amp:\n",
    "            with autocast('cuda'):\n",
    "                loss = sum(model(images, targets).values())\n",
    "        else:\n",
    "            loss = sum(model(images, targets).values())\n",
    "        total += loss.item(); n += 1\n",
    "    return total / max(n, 1)\n",
    "\n",
    "print('Utilities defined.')\n",
    "print('Gradient clipping: max_norm=10.0')\n",
    "print('Validation supports AMP for faster forward pass')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c2e82b",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef5ef8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze backbone initially\n",
    "freeze_backbone(model, freeze=True)\n",
    "\n",
    "# Optimizer with separate param groups\n",
    "head_params = [p for n, p in model.named_parameters() if not n.startswith('backbone') and p.requires_grad]\n",
    "backbone_params = [p for p in model.backbone.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = optim.SGD([\n",
    "    {'params': head_params, 'lr': LR},\n",
    "    {'params': backbone_params, 'lr': LR * 0.1},\n",
    "], momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=LR_MIN)\n",
    "scaler = GradScaler('cuda') if USE_AMP and DEVICE.type == 'cuda' else None\n",
    "\n",
    "best_val = float('inf')\n",
    "best_map = 0.0\n",
    "patience_ctr = 0\n",
    "history = []\n",
    "\n",
    "print(f'Training for {EPOCHS} epochs on {DEVICE}')\n",
    "print(f'AMP: {scaler is not None}, Backbone frozen for first {FREEZE_BACKBONE_EPOCHS} epochs')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5f4dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Unfreeze backbone\n",
    "    if epoch == FREEZE_BACKBONE_EPOCHS + 1:\n",
    "        freeze_backbone(model, freeze=False)\n",
    "        backbone_params = list(model.backbone.parameters())\n",
    "        head_params = [p for n, p in model.named_parameters() if not n.startswith('backbone')]\n",
    "        optimizer = optim.SGD([\n",
    "            {'params': head_params, 'lr': scheduler.get_last_lr()[0]},\n",
    "            {'params': backbone_params, 'lr': scheduler.get_last_lr()[0] * 0.1},\n",
    "        ], momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS - epoch + 1, eta_min=LR_MIN)\n",
    "        print(f'  >> Backbone unfrozen at epoch {epoch}')\n",
    "\n",
    "    tloss = train_one_epoch(model, train_loader, optimizer, scaler, DEVICE, epoch)\n",
    "    vloss = val_loss(model, val_loader, DEVICE, use_amp=USE_AMP)\n",
    "\n",
    "    # mAP every 5 epochs\n",
    "    mAP = 0.0\n",
    "    if epoch % 5 == 0 or epoch == EPOCHS:\n",
    "        mAP, per_cls = compute_map(model, val_loader, DEVICE)\n",
    "        ap_str = ' | '.join(f'{CATEGORIES.get(c,c)}: {ap:.3f}' for c, ap in sorted(per_cls.items()))\n",
    "        print(f'  mAP@0.5: {mAP:.4f}  [{ap_str}]')\n",
    "\n",
    "    scheduler.step()\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    elapsed = time.time() - t0\n",
    "\n",
    "    print(f'Epoch {epoch:3d} | Train: {tloss:.4f} | Val: {vloss:.4f} | mAP: {mAP:.4f} | LR: {lr:.6f} | {elapsed:.1f}s')\n",
    "    history.append({'epoch': epoch, 'train_loss': tloss, 'val_loss': vloss, 'mAP': mAP, 'lr': lr})\n",
    "\n",
    "    # Save best by val loss\n",
    "    if vloss < best_val:\n",
    "        best_val = vloss\n",
    "        patience_ctr = 0\n",
    "        torch.save({'model': model.state_dict(), 'epoch': epoch,\n",
    "                    'best_val': best_val, 'best_map': best_map},\n",
    "                   OUTPUT_DIR / 'best.pth')\n",
    "        print(f'  -> Saved best model (val_loss={vloss:.4f})')\n",
    "    else:\n",
    "        patience_ctr += 1\n",
    "\n",
    "    # Save best by mAP\n",
    "    if mAP > best_map:\n",
    "        best_map = mAP\n",
    "        torch.save({'model': model.state_dict(), 'epoch': epoch, 'best_map': best_map},\n",
    "                   OUTPUT_DIR / 'best_map.pth')\n",
    "        print(f'  -> Saved best mAP model ({mAP:.4f})')\n",
    "\n",
    "    # Full checkpoint every 10 epochs for crash recovery\n",
    "    if epoch % 10 == 0:\n",
    "        torch.save({\n",
    "            'model': model.state_dict(), 'epoch': epoch,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'scaler': scaler.state_dict() if scaler else None,\n",
    "            'best_val': best_val, 'best_map': best_map,\n",
    "            'patience_ctr': patience_ctr, 'history': history,\n",
    "        }, OUTPUT_DIR / f'checkpoint_epoch_{epoch}.pth')\n",
    "        print(f'  -> Checkpoint saved (epoch {epoch}, resumable)')\n",
    "\n",
    "    if patience_ctr >= PATIENCE:\n",
    "        print(f'\\nEarly stopping at epoch {epoch}')\n",
    "        break\n",
    "\n",
    "print(f'\\nDone! Best val_loss: {best_val:.4f}, Best mAP: {best_map:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ce1641",
   "metadata": {},
   "source": [
    "## Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c029a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = [h['epoch'] for h in history]\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(epochs, [h['train_loss'] for h in history], label='Train')\n",
    "ax1.plot(epochs, [h['val_loss'] for h in history], label='Val')\n",
    "ax1.set_xlabel('Epoch'); ax1.set_ylabel('Loss'); ax1.legend(); ax1.set_title('Loss')\n",
    "\n",
    "map_epochs = [h['epoch'] for h in history if h['mAP'] > 0]\n",
    "map_vals = [h['mAP'] for h in history if h['mAP'] > 0]\n",
    "ax2.plot(map_epochs, map_vals, 'go-')\n",
    "ax2.set_xlabel('Epoch'); ax2.set_ylabel('mAP@0.5'); ax2.set_title('mAP')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'training_curves.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767bb71c",
   "metadata": {},
   "source": [
    "## Export ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad77e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and export to ONNX for mobile deployment\n",
    "best_ckpt = torch.load(OUTPUT_DIR / 'best_map.pth', map_location='cpu', weights_only=True)\n",
    "model_export = ssdlite320_mobilenet_v3_large(num_classes=NUM_CLASSES)\n",
    "model_export.load_state_dict(best_ckpt['model'])\n",
    "model_export.eval()\n",
    "\n",
    "dummy = [torch.randn(3, 320, 320)]\n",
    "torch.onnx.export(\n",
    "    model_export, dummy, str(OUTPUT_DIR / 'ssdlite_detection.onnx'),\n",
    "    opset_version=17,\n",
    "    input_names=['image'],\n",
    "    output_names=['boxes', 'labels', 'scores'],\n",
    ")\n",
    "onnx_size = (OUTPUT_DIR / 'ssdlite_detection.onnx').stat().st_size / 1024**2\n",
    "print(f'ONNX exported: {onnx_size:.1f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f56b516",
   "metadata": {},
   "source": [
    "## Save History & Download\n",
    "Download the output files from `/kaggle/working/detection_output/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33b9662",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUTPUT_DIR / 'history.json', 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "# Free VRAM\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print('Output files:')\n",
    "for f in sorted(OUTPUT_DIR.iterdir()):\n",
    "    print(f'  {f.name}: {f.stat().st_size / 1024**2:.1f} MB')\n",
    "print(f'\\nDownload all from: {OUTPUT_DIR}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
