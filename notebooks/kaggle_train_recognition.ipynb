{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5106798",
   "metadata": {},
   "source": [
    "# DrishT: CRNN-Light Text Recognition Training\n",
    "\n",
    "**Model**: CRNN-Light — LightCNN (depthwise sep + SE) + BiLSTM + CTC (~3M params)  \n",
    "**Dataset**: 188,879 train / 23,601 val word crops (722 chars, 13 scripts)  \n",
    "**GPU**: Kaggle T4 (free, 30h/week)  \n",
    "\n",
    "## Setup\n",
    "1. Add dataset `drisht-recognition` to this notebook\n",
    "2. Enable GPU: Settings → Accelerator → GPU T4 x2\n",
    "3. Run all cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d20580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, csv, time, json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import autocast, GradScaler\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e571cf",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea943f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Adjust this path based on your Kaggle dataset name ---\n",
    "DATA_ROOT = Path('/kaggle/input/drisht-recognition')\n",
    "OUTPUT_DIR = Path('/kaggle/working/recognition_output')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TRAIN_CSV = DATA_ROOT / 'train' / 'labels.csv'\n",
    "TRAIN_IMAGES = DATA_ROOT / 'train' / 'images'\n",
    "VAL_CSV = DATA_ROOT / 'val' / 'labels.csv'\n",
    "VAL_IMAGES = DATA_ROOT / 'val' / 'images'\n",
    "CHARSET_FILE = DATA_ROOT / 'charset.txt'\n",
    "\n",
    "# Model\n",
    "IMG_HEIGHT = 32\n",
    "IMG_WIDTH = 128\n",
    "NUM_CHANNELS = 1  # Grayscale\n",
    "HIDDEN_SIZE = 256\n",
    "NUM_LSTM_LAYERS = 2\n",
    "DROPOUT = 0.3\n",
    "CTC_BLANK = 0\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 128  # T4 has 16GB, can handle large batches\n",
    "NUM_WORKERS = 2\n",
    "EPOCHS = 80\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "LR_MIN = 1e-6\n",
    "PATIENCE = 12\n",
    "USE_AMP = True\n",
    "\n",
    "# Verify data\n",
    "for p in [TRAIN_CSV, VAL_CSV, CHARSET_FILE]:\n",
    "    assert p.exists(), f'Missing: {p}'\n",
    "print(f'Train images: {len(list(TRAIN_IMAGES.iterdir()))}')\n",
    "print(f'Val images: {len(list(VAL_IMAGES.iterdir()))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b7805c",
   "metadata": {},
   "source": [
    "## Character Codec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d8f416",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharCodec:\n",
    "    def __init__(self, charset_path):\n",
    "        with open(charset_path, 'r', encoding='utf-8') as f:\n",
    "            chars = [line.strip() for line in f if line.strip()]\n",
    "        self.char_to_idx = {ch: i + 1 for i, ch in enumerate(chars)}\n",
    "        self.idx_to_char = {i + 1: ch for i, ch in enumerate(chars)}\n",
    "        self.num_classes = len(chars) + 1  # +1 for CTC blank\n",
    "\n",
    "    def encode(self, text):\n",
    "        return [self.char_to_idx.get(ch, 0) for ch in text]\n",
    "\n",
    "    def decode(self, indices):\n",
    "        return ''.join(self.idx_to_char.get(idx, '') for idx in indices)\n",
    "\n",
    "\n",
    "codec = CharCodec(CHARSET_FILE)\n",
    "print(f'Charset: {codec.num_classes} classes ({codec.num_classes - 1} chars + blank)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7776fb",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c686193",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size=3, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv2d(in_ch, in_ch, kernel_size, stride, padding, groups=in_ch, bias=False)\n",
    "        self.pointwise = nn.Conv2d(in_ch, out_ch, 1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_ch)\n",
    "        self.relu = nn.ReLU6(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.pointwise(self.depthwise(x))))\n",
    "\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=4):\n",
    "        super().__init__()\n",
    "        mid = max(channels // reduction, 8)\n",
    "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(channels, mid, bias=False), nn.ReLU(inplace=True),\n",
    "            nn.Linear(mid, channels, bias=False), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.squeeze(x).view(b, c)\n",
    "        return x * self.excitation(y).view(b, c, 1, 1)\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, stride=1, expand_ratio=4, use_se=True):\n",
    "        super().__init__()\n",
    "        mid_ch = in_ch * expand_ratio\n",
    "        self.use_residual = (stride == 1 and in_ch == out_ch)\n",
    "        layers = []\n",
    "        if expand_ratio != 1:\n",
    "            layers += [nn.Conv2d(in_ch, mid_ch, 1, bias=False), nn.BatchNorm2d(mid_ch), nn.ReLU6(inplace=True)]\n",
    "        layers += [nn.Conv2d(mid_ch, mid_ch, 3, stride, 1, groups=mid_ch, bias=False),\n",
    "                   nn.BatchNorm2d(mid_ch), nn.ReLU6(inplace=True)]\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "        self.se = SEBlock(mid_ch) if use_se else nn.Identity()\n",
    "        self.project = nn.Sequential(nn.Conv2d(mid_ch, out_ch, 1, bias=False), nn.BatchNorm2d(out_ch))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.project(self.se(self.conv(x)))\n",
    "        return out + x if self.use_residual else out\n",
    "\n",
    "\n",
    "class LightCNNEncoder(nn.Module):\n",
    "    \"\"\"(B, C, 32, W) -> (B, W/2, 512)\"\"\"\n",
    "    def __init__(self, in_channels=1):\n",
    "        super().__init__()\n",
    "        self.stage1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, 3, padding=1, bias=False), nn.BatchNorm2d(32), nn.ReLU6(inplace=True),\n",
    "            DepthwiseSeparableConv(32, 64), nn.MaxPool2d((2, 1)))\n",
    "        self.stage2 = nn.Sequential(\n",
    "            InvertedResidual(64, 128), InvertedResidual(128, 128), nn.MaxPool2d((2, 1)))\n",
    "        self.stage3 = nn.Sequential(\n",
    "            InvertedResidual(128, 256), InvertedResidual(256, 256), nn.MaxPool2d((2, 2)))\n",
    "        self.stage4 = nn.Sequential(\n",
    "            InvertedResidual(256, 384), InvertedResidual(384, 384), nn.MaxPool2d((2, 1)), nn.Dropout2d(0.1))\n",
    "        self.stage5 = nn.Sequential(\n",
    "            nn.Conv2d(384, 512, (2, 1), bias=False), nn.BatchNorm2d(512), nn.ReLU6(inplace=True),\n",
    "            SEBlock(512), nn.Dropout2d(0.1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        x = self.stage5(x)\n",
    "        return x.squeeze(2).permute(0, 2, 1)  # (B, W/2, 512)\n",
    "\n",
    "\n",
    "class BiLSTMSequenceModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, bidirectional=True, batch_first=True,\n",
    "                           dropout=dropout if num_layers > 1 else 0)\n",
    "        self.linear = nn.Linear(hidden_size * 2, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(self.lstm(x)[0])\n",
    "\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, num_classes, img_h=32, num_channels=1):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.img_h = img_h\n",
    "        self.num_channels = num_channels\n",
    "        self.cnn = LightCNNEncoder(in_channels=num_channels)\n",
    "        self.rnn = BiLSTMSequenceModel(512, HIDDEN_SIZE, NUM_LSTM_LAYERS, DROPOUT)\n",
    "        self.output = nn.Linear(HIDDEN_SIZE, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.output(self.rnn(self.cnn(x))), dim=2)\n",
    "\n",
    "    def decode_greedy(self, log_probs):\n",
    "        _, preds = log_probs.max(dim=2)\n",
    "        results = []\n",
    "        for b in range(preds.size(0)):\n",
    "            decoded, prev = [], -1\n",
    "            for p in preds[b].tolist():\n",
    "                if p != prev and p != CTC_BLANK:\n",
    "                    decoded.append(p)\n",
    "                prev = p\n",
    "            results.append(decoded)\n",
    "        return results\n",
    "\n",
    "\n",
    "model = CRNN(num_classes=codec.num_classes, img_h=IMG_HEIGHT, num_channels=NUM_CHANNELS).to(DEVICE)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f'CRNN-Light: {total:,} params, {sum(p.numel()*p.element_size() for p in model.parameters())/1024**2:.1f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8ad34c",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67cc4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecognitionDataset(Dataset):\n",
    "    def __init__(self, csv_path, img_dir, codec, img_h=32, img_w=128, num_channels=1, augment=False):\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.codec = codec\n",
    "        self.img_h, self.img_w = img_h, img_w\n",
    "        self.num_channels = num_channels\n",
    "        self.augment = augment\n",
    "        self.samples = []\n",
    "        with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "            for row in csv.DictReader(f):\n",
    "                label = row['label'].strip()\n",
    "                if not label or label == 'UNK': continue\n",
    "                encoded = self.codec.encode(label)\n",
    "                if any(idx == 0 for idx in encoded): continue\n",
    "                self.samples.append((row['image'], label))\n",
    "        print(f'  Loaded {len(self.samples)} from {csv_path}')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name, label = self.samples[idx]\n",
    "        try:\n",
    "            mode = 'L' if self.num_channels == 1 else 'RGB'\n",
    "            image = Image.open(self.img_dir / img_name).convert(mode)\n",
    "        except Exception:\n",
    "            image = Image.new('L' if self.num_channels == 1 else 'RGB',\n",
    "                             (self.img_w, self.img_h), 128)\n",
    "            label = ''\n",
    "\n",
    "        # Aspect-ratio-preserving resize + right-pad\n",
    "        w, h = image.size\n",
    "        new_w = min(int(w * self.img_h / h), self.img_w)\n",
    "        image = image.resize((new_w, self.img_h), Image.BILINEAR)\n",
    "        padded = Image.new('L' if self.num_channels == 1 else 'RGB', (self.img_w, self.img_h), 0)\n",
    "        padded.paste(image, (0, 0))\n",
    "\n",
    "        if self.augment:\n",
    "            import random\n",
    "            if random.random() < 0.3:\n",
    "                padded = T.functional.adjust_brightness(padded, random.uniform(0.7, 1.3))\n",
    "            if random.random() < 0.2:\n",
    "                padded = T.functional.adjust_contrast(padded, random.uniform(0.8, 1.2))\n",
    "\n",
    "        tensor = T.ToTensor()(padded)\n",
    "        tensor = (tensor - 0.5) / 0.5 if self.num_channels == 1 else T.Normalize([.485,.456,.406],[.229,.224,.225])(tensor)\n",
    "\n",
    "        encoded = self.codec.encode(label)\n",
    "        return tensor, torch.tensor(encoded, dtype=torch.long), torch.tensor(len(encoded), dtype=torch.long)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, targets, lengths = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    max_len = max(t.size(0) for t in targets) if targets[0].size(0) > 0 else 1\n",
    "    padded = torch.zeros(len(targets), max_len, dtype=torch.long)\n",
    "    for i, t in enumerate(targets):\n",
    "        if t.size(0) > 0: padded[i, :t.size(0)] = t\n",
    "    return images, padded, torch.stack(lengths)\n",
    "\n",
    "\n",
    "train_ds = RecognitionDataset(TRAIN_CSV, TRAIN_IMAGES, codec, IMG_HEIGHT, IMG_WIDTH, NUM_CHANNELS, augment=True)\n",
    "val_ds = RecognitionDataset(VAL_CSV, VAL_IMAGES, codec, IMG_HEIGHT, IMG_WIDTH, NUM_CHANNELS, augment=False)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=True)\n",
    "\n",
    "print(f'Train: {len(train_ds)}, Val: {len(val_ds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edea52ab",
   "metadata": {},
   "source": [
    "## Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea2f5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(model, loader, codec, device, max_batches=50):\n",
    "    model.eval()\n",
    "    total_chars, correct_chars, total_words, correct_words = 0, 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for i, (images, targets, lengths) in enumerate(loader):\n",
    "            if i >= max_batches: break\n",
    "            log_probs = model(images.to(device))\n",
    "            decoded = model.decode_greedy(log_probs)\n",
    "            for b in range(len(decoded)):\n",
    "                pred = codec.decode(decoded[b])\n",
    "                gt = codec.decode(targets[b, :lengths[b]].tolist())\n",
    "                total_words += 1\n",
    "                if pred == gt: correct_words += 1\n",
    "                for p, g in zip(pred, gt):\n",
    "                    total_chars += 1\n",
    "                    if p == g: correct_chars += 1\n",
    "                total_chars += abs(len(pred) - len(gt))\n",
    "    return correct_chars / max(total_chars, 1) * 100, correct_words / max(total_words, 1) * 100\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, scaler, device, epoch):\n",
    "    model.train()\n",
    "    ctc = nn.CTCLoss(blank=CTC_BLANK, zero_infinity=True)\n",
    "    total, n = 0.0, 0\n",
    "    pbar = tqdm(loader, desc=f'Epoch {epoch}')\n",
    "    for images, targets, lengths in pbar:\n",
    "        images, targets, lengths = images.to(device), targets.to(device), lengths.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        if scaler:\n",
    "            with autocast('cuda'):\n",
    "                log_probs = model(images).permute(1, 0, 2)\n",
    "                input_lengths = torch.full((images.size(0),), log_probs.size(0), dtype=torch.long, device=device)\n",
    "                loss = ctc(log_probs, targets, input_lengths, lengths)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            log_probs = model(images).permute(1, 0, 2)\n",
    "            input_lengths = torch.full((images.size(0),), log_probs.size(0), dtype=torch.long, device=device)\n",
    "            loss = ctc(log_probs, targets, input_lengths, lengths)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "        total += loss.item(); n += 1\n",
    "        pbar.set_postfix(loss=f'{loss.item():.4f}')\n",
    "    return total / max(n, 1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, device):\n",
    "    model.eval()\n",
    "    ctc = nn.CTCLoss(blank=CTC_BLANK, zero_infinity=True)\n",
    "    total, n = 0.0, 0\n",
    "    for images, targets, lengths in tqdm(loader, desc='Val'):\n",
    "        images, targets, lengths = images.to(device), targets.to(device), lengths.to(device)\n",
    "        log_probs = model(images).permute(1, 0, 2)\n",
    "        input_lengths = torch.full((images.size(0),), log_probs.size(0), dtype=torch.long, device=device)\n",
    "        loss = ctc(log_probs, targets, input_lengths, lengths)\n",
    "        total += loss.item(); n += 1\n",
    "    return total / max(n, 1)\n",
    "\n",
    "print('Utilities defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7731ca9c",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdcca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=LR, betas=(0.9, 0.999), weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=LR_MIN)\n",
    "scaler = GradScaler('cuda') if USE_AMP and DEVICE.type == 'cuda' else None\n",
    "\n",
    "best_val = float('inf')\n",
    "best_word_acc = 0.0\n",
    "patience_ctr = 0\n",
    "history = []\n",
    "\n",
    "print(f'Training CRNN-Light for {EPOCHS} epochs on {DEVICE}')\n",
    "print(f'AMP: {scaler is not None}, Batch: {BATCH_SIZE}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5a63b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    t0 = time.time()\n",
    "\n",
    "    tloss = train_one_epoch(model, train_loader, optimizer, scaler, DEVICE, epoch)\n",
    "    vloss = validate(model, val_loader, DEVICE)\n",
    "\n",
    "    char_acc, word_acc = compute_metrics(model, val_loader, codec, DEVICE)\n",
    "\n",
    "    scheduler.step()\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    elapsed = time.time() - t0\n",
    "\n",
    "    print(f'Epoch {epoch:3d} | Train: {tloss:.4f} | Val: {vloss:.4f} | '\n",
    "          f'Char: {char_acc:.1f}% | Word: {word_acc:.1f}% | LR: {lr:.6f} | {elapsed:.1f}s')\n",
    "    history.append({'epoch': epoch, 'train_loss': tloss, 'val_loss': vloss,\n",
    "                    'char_acc': char_acc, 'word_acc': word_acc, 'lr': lr})\n",
    "\n",
    "    # Save best by val loss\n",
    "    if vloss < best_val:\n",
    "        best_val = vloss\n",
    "        patience_ctr = 0\n",
    "        torch.save({'state_dict': model.state_dict(), 'num_classes': codec.num_classes,\n",
    "                    'img_h': IMG_HEIGHT, 'num_channels': NUM_CHANNELS,\n",
    "                    'epoch': epoch, 'best_val': best_val},\n",
    "                   OUTPUT_DIR / 'best.pth')\n",
    "        print(f'  -> Saved best model (val={vloss:.4f})')\n",
    "    else:\n",
    "        patience_ctr += 1\n",
    "\n",
    "    # Save best by word accuracy\n",
    "    if word_acc > best_word_acc:\n",
    "        best_word_acc = word_acc\n",
    "        torch.save({'state_dict': model.state_dict(), 'num_classes': codec.num_classes,\n",
    "                    'img_h': IMG_HEIGHT, 'num_channels': NUM_CHANNELS,\n",
    "                    'epoch': epoch, 'best_word_acc': best_word_acc},\n",
    "                   OUTPUT_DIR / 'best_acc.pth')\n",
    "        print(f'  -> Saved best accuracy model (word={word_acc:.1f}%)')\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        torch.save({'state_dict': model.state_dict(), 'num_classes': codec.num_classes,\n",
    "                    'img_h': IMG_HEIGHT, 'num_channels': NUM_CHANNELS, 'epoch': epoch},\n",
    "                   OUTPUT_DIR / f'epoch_{epoch}.pth')\n",
    "\n",
    "    if patience_ctr >= PATIENCE:\n",
    "        print(f'\\nEarly stopping at epoch {epoch}')\n",
    "        break\n",
    "\n",
    "print(f'\\nDone! Best val: {best_val:.4f}, Best word acc: {best_word_acc:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8b0ea3",
   "metadata": {},
   "source": [
    "## Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e034c2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "eps = [h['epoch'] for h in history]\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].plot(eps, [h['train_loss'] for h in history], label='Train')\n",
    "axes[0].plot(eps, [h['val_loss'] for h in history], label='Val')\n",
    "axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('CTC Loss'); axes[0].legend(); axes[0].set_title('Loss')\n",
    "\n",
    "axes[1].plot(eps, [h['char_acc'] for h in history], 'b-')\n",
    "axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('%'); axes[1].set_title('Character Accuracy')\n",
    "\n",
    "axes[2].plot(eps, [h['word_acc'] for h in history], 'g-')\n",
    "axes[2].set_xlabel('Epoch'); axes[2].set_ylabel('%'); axes[2].set_title('Word Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'training_curves.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117f4e22",
   "metadata": {},
   "source": [
    "## Export ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6a121e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for export\n",
    "ckpt = torch.load(OUTPUT_DIR / 'best_acc.pth', map_location='cpu')\n",
    "model_export = CRNN(ckpt['num_classes'], ckpt.get('img_h', 32), ckpt.get('num_channels', 1))\n",
    "model_export.load_state_dict(ckpt['state_dict'])\n",
    "model_export.eval()\n",
    "\n",
    "dummy = torch.randn(1, NUM_CHANNELS, IMG_HEIGHT, IMG_WIDTH)\n",
    "torch.onnx.export(\n",
    "    model_export, dummy, str(OUTPUT_DIR / 'crnn_recognition.onnx'),\n",
    "    opset_version=17,\n",
    "    input_names=['image'],\n",
    "    output_names=['log_probs'],\n",
    "    dynamic_axes={'image': {0: 'batch', 3: 'width'}, 'log_probs': {0: 'batch', 1: 'timesteps'}},\n",
    ")\n",
    "onnx_size = (OUTPUT_DIR / 'crnn_recognition.onnx').stat().st_size / 1024**2\n",
    "print(f'ONNX exported: {onnx_size:.1f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe983f9",
   "metadata": {},
   "source": [
    "## Save & Download\n",
    "Download from `/kaggle/working/recognition_output/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4b9e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUTPUT_DIR / 'history.json', 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "# Copy charset for inference\n",
    "import shutil\n",
    "shutil.copy2(CHARSET_FILE, OUTPUT_DIR / 'charset.txt')\n",
    "\n",
    "print('Output files:')\n",
    "for f in sorted(OUTPUT_DIR.iterdir()):\n",
    "    print(f'  {f.name}: {f.stat().st_size / 1024**2:.1f} MB')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
